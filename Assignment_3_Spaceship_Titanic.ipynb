{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucdekeijzer/Assignment_3_Spaceship_Titanic/blob/main/Assignment_3_Spaceship_Titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')"
      ],
      "metadata": {
        "id": "KzW86UsUjzqA"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'spaceship-titanic'\n",
        "\n",
        "!kaggle competitions download -c $dataset\n",
        "\n",
        "zip_file = f\"{dataset}.zip\"\n",
        "destination_dir = f\"/content/{dataset}\"\n",
        "\n",
        "if not os.path.exists(zip_file):\n",
        "    print(f\"Error: {zip_file} not found.\")\n",
        "else:\n",
        "    !unzip -q $zip_file -d $destination_dir\n",
        "    !rm $zip_file"
      ],
      "metadata": {
        "id": "iBzz141kkVgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# used for debugging\n",
        "!pip install -Uqq ipdb\n",
        "import ipdb\n",
        "%pdb on\n",
        "#ipdb.set_trace()"
      ],
      "metadata": {
        "id": "CJGbgEtk_9rv",
        "outputId": "6a478385-56c8-48ec-83db-7909e06b37be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'import warnings' failed; traceback:\n",
            "\n",
            "Automatic pdb calling has been turned ON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjadEwb0iwpH",
        "outputId": "5305e2b7-4f54-4b0f-9642-5de17059bc1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n",
            "0        0001_01     Europa     False     B/0/P    TRAPPIST-1e  39.0  False   \n",
            "1        0002_01      Earth     False     F/0/S    TRAPPIST-1e  24.0  False   \n",
            "2        0003_01     Europa     False     A/0/S    TRAPPIST-1e  58.0   True   \n",
            "3        0003_02     Europa     False     A/0/S    TRAPPIST-1e  33.0  False   \n",
            "4        0004_01      Earth     False     F/1/S    TRAPPIST-1e  16.0  False   \n",
            "...          ...        ...       ...       ...            ...   ...    ...   \n",
            "8688     9276_01     Europa     False    A/98/P    55 Cancri e  41.0   True   \n",
            "8689     9278_01      Earth      True  G/1499/S  PSO J318.5-22  18.0  False   \n",
            "8690     9279_01      Earth     False  G/1500/S    TRAPPIST-1e  26.0  False   \n",
            "8691     9280_01     Europa     False   E/608/S    55 Cancri e  32.0  False   \n",
            "8692     9280_02     Europa     False   E/608/S    TRAPPIST-1e  44.0  False   \n",
            "\n",
            "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
            "0             0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
            "1           109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
            "2            43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
            "3             0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
            "4           303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
            "...           ...        ...           ...     ...     ...                ...   \n",
            "8688          0.0     6819.0           0.0  1643.0    74.0  Gravior Noxnuther   \n",
            "8689          0.0        0.0           0.0     0.0     0.0    Kurta Mondalley   \n",
            "8690          0.0        0.0        1872.0     1.0     0.0       Fayey Connon   \n",
            "8691          0.0     1049.0           0.0   353.0  3235.0   Celeon Hontichre   \n",
            "8692        126.0     4688.0           0.0     0.0    12.0   Propsh Hontichre   \n",
            "\n",
            "      Transported  \n",
            "0           False  \n",
            "1            True  \n",
            "2           False  \n",
            "3           False  \n",
            "4            True  \n",
            "...           ...  \n",
            "8688        False  \n",
            "8689        False  \n",
            "8690         True  \n",
            "8691        False  \n",
            "8692         True  \n",
            "\n",
            "[8693 rows x 14 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "train_data_df = pd.read_csv(\"/content/spaceship-titanic/train.csv\")\n",
        "test_data_df = pd.read_csv(\"/content/spaceship-titanic/test.csv\")\n",
        "print(train_data_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wzo3zse-oc7M",
        "outputId": "58d084e7-7182-4424-f022-3df3febeee5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId       0\n",
              "HomePlanet      201\n",
              "CryoSleep       217\n",
              "Cabin           199\n",
              "Destination     182\n",
              "Age             179\n",
              "VIP             203\n",
              "RoomService     181\n",
              "FoodCourt       183\n",
              "ShoppingMall    208\n",
              "Spa             183\n",
              "VRDeck          188\n",
              "Name            200\n",
              "Transported       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "train_data_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8JrjakFpimS",
        "outputId": "8e3d8a94-8d95-4684-aebb-29c2ef559cfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8693\n",
            "8693\n"
          ]
        }
      ],
      "source": [
        "print(len(train_data_df))\n",
        "train_data_df_filled_na = train_data_df.fillna(method = \"ffill\")\n",
        "print(len(train_data_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gySVAlc5qJup",
        "outputId": "4d3de7f8-267f-4e58-9ca5-b199e2201b31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId       0\n",
              "HomePlanet      201\n",
              "CryoSleep       217\n",
              "Cabin           199\n",
              "Destination     182\n",
              "Age             179\n",
              "VIP             203\n",
              "RoomService     181\n",
              "FoodCourt       183\n",
              "ShoppingMall    208\n",
              "Spa             183\n",
              "VRDeck          188\n",
              "Name            200\n",
              "Transported       0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "train_data_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vthmW0o5wYk2",
        "outputId": "3e8dc806-0b79-43bd-a197-bb18fc249c0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PassengerId      object\n",
            "HomePlanet       object\n",
            "CryoSleep          bool\n",
            "Cabin            object\n",
            "Destination      object\n",
            "Age             float64\n",
            "VIP                bool\n",
            "RoomService     float64\n",
            "FoodCourt       float64\n",
            "ShoppingMall    float64\n",
            "Spa             float64\n",
            "VRDeck          float64\n",
            "Name             object\n",
            "Transported        bool\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(train_data_df_filled_na.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "R3ffg11ojc4b"
      },
      "outputs": [],
      "source": [
        "len(train_data_df_filled_na)\n",
        "\n",
        "X = train_data_df_filled_na.loc[:, train_data_df.columns != 'Transported']\n",
        "y = train_data_df_filled_na['Transported']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "exp_feats=['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
        "\n",
        "#added the features expenditure and no spending which is based on the persons spendings on the ship\n",
        "X['amenities']=X[exp_feats].sum(axis=1)\n",
        "X['No_spending']=(X['amenities']==0).astype(int)\n",
        "\n",
        "# Plot distribution of new features\n",
        "fig=plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "sns.histplot(data=X, x='amenities', hue='Transported', bins=200)\n",
        "plt.title('Total amenities (truncated)')\n",
        "plt.ylim([0,200])\n",
        "plt.xlim([0,20000])\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.countplot(data=X, x='No_spending', hue='Transported')\n",
        "plt.title('No spending indicator')\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "hZ-850tGTvYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fgjZGWE_Z27"
      },
      "outputs": [],
      "source": [
        "# Encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encoding\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# oneHotEncoder = OneHotEncoder(handle_unknown='ignore')\n",
        "# # encode the entire training dataset\n",
        "# columns_to_encode = [\"PassengerId\", \"HomePlanet\", \"Destination\", \"Cabin\", \"Name\"]\n",
        "# oneHotEncoder.fit(X[columns_to_encode])\n",
        "# # encode the specified columns individually\n",
        "# X[columns_to_encode] = oneHotEncoder.transform(X[columns_to_encode])\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "X['PassengerId'] = label_encoder.fit_transform(X['PassengerId'])\n",
        "X[\"HomePlanet\"] = label_encoder.fit_transform(X[\"HomePlanet\"])\n",
        "X['Destination'] = label_encoder.fit_transform(X['Destination'])\n",
        "X['Cabin'] = label_encoder.fit_transform(X['Cabin'])\n",
        "X['Name'] = label_encoder.fit_transform(X['Name'])\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6CNLqSHFYu6"
      },
      "outputs": [],
      "source": [
        "print(X.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Enqems_WmGgb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODOq5UWSFr2a"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "print(X_train[\"RoomService\"].shape)\n",
        "\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "columns_to_transform = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n",
        "\n",
        "# Fit scaler to the entire training dataset\n",
        "scaler.fit(X_train[columns_to_transform])\n",
        "\n",
        "# Transform both Train and Validation\n",
        "X_train[columns_to_transform] = scaler.transform(X_train[columns_to_transform])\n",
        "X_val[columns_to_transform] = scaler.transform(X_val[columns_to_transform])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcGh448uRAv8"
      },
      "outputs": [],
      "source": [
        "# Perform PCA\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# pca = PCA()  # initializing the pca\n",
        "# pca.set_params(n_components = .99)    # configure the number of components\n",
        "# X_train = pca.fit_transform(X_train)\n",
        "# X_val = pca.transform(X_val)\n",
        "\n",
        "classifier = SVC()\n",
        "\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# Initialize SVC\n",
        "svc = SVC()\n",
        "\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "# grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=3)\n",
        "\n",
        "# # Perform grid search on training data\n",
        "# grid_search.fit(X_train[:50], y_train[:50])\n",
        "\n",
        "# # Print best parameters and best score\n",
        "# print(\"Best Parameters:\", grid_search.best_params_)\n",
        "# print(\"Best Score:\", grid_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(svc.score(X_train, y_train))\n",
        "print(svc.score(X_val, y_val))"
      ],
      "metadata": {
        "id": "W7EdQ3Ahbu6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoapzNmIKlVG"
      },
      "outputs": [],
      "source": [
        "print(classifier.score(X_train, y_train))\n",
        "print(classifier.score(X_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "prediction = model.predict(X_val)\n",
        "\n",
        "correct_pred = (prediction == y_val).sum()\n",
        "total_pred = len(y_val)\n",
        "\n",
        "acc = correct_pred/total_pred\n",
        "print(\"Accuracy:\", acc)"
      ],
      "metadata": {
        "id": "36XKWm1Ph1_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = []\n",
        "depth = [i for i in range(1, 21)]\n",
        "\n",
        "for d in depth:\n",
        "    model = DecisionTreeClassifier(random_state = 42, max_depth=d)\n",
        "    model.fit(X_train, y_train)\n",
        "    prediction = model.predict(X_val)\n",
        "    correct_pred = (prediction == y_val).sum()\n",
        "    total_pred = len(y_val)\n",
        "\n",
        "    acc.append(correct_pred/total_pred)\n",
        "\n",
        "\n",
        "plt.plot(depth, acc)\n",
        "plt.xlabel(\"Depth\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.title(\"accuracy vs max depth\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "EE0aX231qvG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(random_state=42, max_depth=10)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "prediction = model.predict(X_val)\n",
        "correct_pred = (prediction == y_val).sum()\n",
        "total_pred = len(y_val)\n",
        "\n",
        "acc = (correct_pred/total_pred)\n",
        "print(\"Accuracy:\", acc)"
      ],
      "metadata": {
        "id": "Hg8R4IvI4mAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "\n",
        "# randomforest with gridsearch\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "  'max_depth': [10, 15],\n",
        "  'n_estimators': [200, 250],  # Number of trees in the forest\n",
        "  'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Perform Grid Search Cross Validation\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the corresponding accuracy\n",
        "best_params = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Accuracy:\", best_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "x25v1gqwuuRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "model = GradientBoostingClassifier(max_depth=7, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(model.score(X_val, y_val))\n",
        "\n",
        "importances = model.feature_importances_\n",
        "\n",
        "feature_names = train_data_df.columns.tolist()\n",
        "\n",
        "importance_feature_pairs = zip(importances, feature_names)\n",
        "\n",
        "# Sort the pairs based on importance values in ascending order\n",
        "sorted_pairs = sorted(importance_feature_pairs)\n",
        "\n",
        "# Print the sorted feature importances\n",
        "for importance, feature_name in sorted_pairs:\n",
        "    print(f\"{feature_name}: importance = {importance}\")"
      ],
      "metadata": {
        "id": "FHkRq3cvz7y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "def correlation_heatmap(df):\n",
        "    _ , ax = plt.subplots(figsize =(14, 12))\n",
        "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
        "\n",
        "    _ = sns.heatmap(\n",
        "        df.corr(),\n",
        "        cmap = colormap,\n",
        "        square=True,\n",
        "        cbar_kws={'shrink':.9 },\n",
        "        ax=ax,\n",
        "        annot=True,\n",
        "        linewidths=0.1,vmax=1.0, linecolor='white',\n",
        "        annot_kws={'fontsize': 5 }\n",
        "    )\n",
        "\n",
        "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
        "\n",
        "correlation_heatmap(train_data_df_filled_na)"
      ],
      "metadata": {
        "id": "fGXYc3TnEh5P"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}